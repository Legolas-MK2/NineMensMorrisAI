# pip install open_spiel torch
import math, random, time, csv
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import pyspiel

# ---------- helpers ----------
def legal_action_mask(state, num_actions):
    """Generates a 1.0/0.0 mask for legal actions."""
    mask = np.zeros(num_actions, dtype=np.float32)
    la = state.legal_actions()  # list[int]
    if la:
        mask[la] = 1.0
    return torch.from_numpy(mask)

def masked_categorical(logits, mask):
    """
    Samples from a categorical distribution, masking out illegal actions.
    logits: [B, A], mask: [B, A] with 0/1 (or [A], [A])
    """
    # Put -inf on illegal actions
    very_neg = torch.finfo(logits.dtype).min
    masked_logits = torch.where(mask > 0, logits, torch.full_like(logits, very_neg))
    return torch.distributions.Categorical(logits=masked_logits)

# ---------- model ----------
class ActorCritic(nn.Module):
    """
    A simple Actor-Critic network with two heads (policy, value).
    """
    def __init__(self, obs_size, num_actions, hidden=256, dropout=0.0):
        super().__init__()
        self.fc1 = nn.Linear(obs_size, hidden)
        self.ln1 = nn.LayerNorm(hidden)
        self.fc2 = nn.Linear(hidden, hidden)
        self.ln2 = nn.LayerNorm(hidden)
        self.pi = nn.Linear(hidden, num_actions)  # policy head
        self.v = nn.Linear(hidden, 1)          # value head
        self.do = nn.Dropout(dropout)

        # Small init for policy head helps early entropy
        nn.init.orthogonal_(self.fc1.weight, math.sqrt(2))
        nn.init.orthogonal_(self.fc2.weight, math.sqrt(2))
        nn.init.zeros_(self.fc1.bias); nn.init.zeros_(self.fc2.bias)
        nn.init.zeros_(self.pi.bias);  nn.init.zeros_(self.v.bias)
        nn.init.orthogonal_(self.pi.weight, 0.01)
        nn.init.orthogonal_(self.v.weight, 1.0)

    def forward(self, obs):
        # obs: [B, obs_size]
        x = self.ln1(F.relu(self.fc1(obs)))
        x = self.do(x)
        x = self.ln2(F.relu(self.fc2(x)))
        x = self.do(x)
        logits = self.pi(x)             # [B, A]
        value = self.v(x).squeeze(-1)   # [B]
        return logits, value

# ---------- training loop (Batched A2C) ----------
def train(n_episodes=2000, 
          lr=3e-4, 
          entropy_coef=0.01, 
          value_coef=0.5, 
          gamma=1.0, 
          n_episodes_per_batch=16, 
          hidden=256, 
          dropout=0.1):
    """
    Trains the agent using batched A2C (REINFORCE with baseline).
    """
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")
    game = pyspiel.load_game("nine_mens_morris")
    obs_size = game.observation_tensor_size()
    num_actions = game.num_distinct_actions()

    # --- IMPROVEMENT: Larger network ---
    net = ActorCritic(obs_size, num_actions, hidden=hidden, dropout=dropout).to(device)
    # --- IMPROVEMENT: Switched to Adam optimizer ---
    opt = torch.optim.Adam(net.parameters(), lr=lr)

    ema_return = None
    time_start = time.perf_counter()
    
    # --- IMPROVEMENT: Batching loop ---
    # We loop by 'n_episodes_per_batch' and perform one update per batch
    for ep_batch_start in range(1, n_episodes + 1, n_episodes_per_batch):
        
        # --- Storage for the whole batch ---
        batch_policy_loss = torch.tensor(0.0, device=device)
        batch_value_loss = torch.tensor(0.0, device=device)
        batch_entropy = torch.tensor(0.0, device=device)
        last_final_returns = [0.0, 0.0] # For logging

        # --- NEW: Vectorized Batch Collection ---

        # --- 1. Setup parallel environments ---
        # (Filter out extra episodes if n_episodes isn't divisible by batch size)
        current_batch_size = min(n_episodes_per_batch, n_episodes - (ep_batch_start - 1))
        if current_batch_size <= 0:
            break

        states = [game.new_initial_state() for _ in range(current_batch_size)]
        
        # --- 2. Setup active trajectory storage ---
        # active_trajs[env_idx][pid] = {'logp': [], 'v': [], 'entropy': []}
        active_trajs = [ {0: {'logp': [], 'v': [], 'entropy': []}, 
                          1: {'logp': [], 'v': [], 'entropy': []}} 
                         for _ in range(current_batch_size) ]
        
        final_returns_list = [None] * current_batch_size
        active_env_indices = list(range(current_batch_size))

        # --- 3. Vectorized Rollout Loop ---
        # Keep looping as long as at least one game is not done
        while active_env_indices:
            # --- 3a. Collect observations from all *active* games ---
            current_pids = []
            obs_list = []
            mask_list = []
            for i in active_env_indices:
                pid = states[i].current_player()
                current_pids.append(pid)
                obs_list.append(torch.tensor(states[i].observation_tensor(pid), dtype=torch.float32))
                mask_list.append(legal_action_mask(states[i], num_actions))

            # --- 3b. Batch and run network (This is the GPU work!) ---
            obs_batch = torch.stack(obs_list).to(device)    # [B_active, obs_size]
            mask_batch = torch.stack(mask_list).to(device)  # [B_active, A]
            
            logits_batch, value_batch = net(obs_batch)      # [B_active, A], [B_active]

            # --- 3c. Batch sample actions ---
            dist = masked_categorical(logits_batch, mask_batch) # Batched distribution
            actions_batch = dist.sample()       # [B_active]
            logps_batch = dist.log_prob(actions_batch)   # [B_active]
            entropies_batch = dist.entropy()  # [B_active]

            # --- 3d. Apply actions and store results ---
            next_active_env_indices = []
            for i, batch_idx in enumerate(active_env_indices):
                # Get the data for this specific environment from the batch
                env_action = actions_batch[i]
                env_logp = logps_batch[i]
                env_value = value_batch[i]
                env_entropy = entropies_batch[i]
                env_pid = current_pids[i]

                # Store in this env's trajectory
                active_trajs[batch_idx][env_pid]['logp'].append(env_logp)
                active_trajs[batch_idx][env_pid]['v'].append(env_value)
                active_trajs[batch_idx][env_pid]['entropy'].append(env_entropy)

                # Step the environment
                states[batch_idx].apply_action(int(env_action.item()))

                # --- 3e. Check if this env is done ---
                if states[batch_idx].is_terminal():
                    # Game is over, save the final returns
                    final_returns_list[batch_idx] = states[batch_idx].returns()
                else:
                    # Game is not over, add it to the list for the *next* iteration
                    next_active_env_indices.append(batch_idx)

            # Update the list of active environments
            active_env_indices = next_active_env_indices
        
        # --- 4. End of Vectorized Rollout ---

        # --- 5. Accumulate losses from all finished trajectories ---
        batch_episodes = current_batch_size
        for i in range(current_batch_size):
            last_final_returns = final_returns_list[i] # For logging
            if last_final_returns is None: continue # Should not happen if loop is correct

            if ema_return is None: ema_return = last_final_returns[0]
            else: ema_return = 0.95 * ema_return + 0.05 * last_final_returns[0]

            for pid in [0, 1]:
                R = torch.tensor(last_final_returns[pid], dtype=torch.float32, device=device)
                traj_data = active_trajs[i][pid]
                
                if len(traj_data['logp']) == 0:
                    continue
                
                logps = torch.stack(traj_data['logp'])
                values = torch.stack(traj_data['v'])
                entropies = torch.stack(traj_data['entropy'])
                
                adv = (R - values.detach())
                batch_policy_loss += -(logps * adv).mean()
                batch_value_loss += F.mse_loss(values, R.expand_as(values))
                batch_entropy += entropies.mean()

        # --- End of Batch Collection ---
        if batch_episodes == 0:
            continue # This should only happen if current_batch_size was 0

        # --- Update Step ---
        # Average the losses over the batch
        avg_policy_loss = batch_policy_loss / batch_episodes
        avg_value_loss = batch_value_loss / batch_episodes
        avg_entropy = batch_entropy / batch_episodes

        total_loss = avg_policy_loss + \
                     value_coef * avg_value_loss - \
                     entropy_coef * avg_entropy

        opt.zero_grad()
        total_loss.backward()
        nn.utils.clip_grad_norm_(net.parameters(), 1.0)
        opt.step()

        # --- Logging (Preserved original style) ---
        # Log roughly every 50 episodes
        current_ep_end = ep_batch_start + batch_episodes - 1
        # Log if this batch *crosses* a 50-episode boundary
        if current_ep_end == 1 or (current_ep_end // 50) > ((ep_batch_start - 1) // 50):
            eta_elapsed = time.perf_counter() - time_start
            eta_remaining = (eta_elapsed / current_ep_end) * (n_episodes - current_ep_end) if current_ep_end > 0 else 0

            eta_s = int(eta_remaining)  # total seconds as int
            eta_h, rem = divmod(eta_s, 3600)    # hours, remainder
            eta_m, eta_s = divmod(rem, 60)      # minutes, seconds

            log_path = f"logs/{str_t}.csv"
            fields = ["ep", "last_return_P0", "ema_return", "loss", "eta"]

            write_header = not os.path.exists(log_path)

            log_f = open(log_path, "a", newline="")
            writer = csv.DictWriter(log_f, fieldnames=fields)
            if write_header:
                writer.writeheader()

            # in loop:
            eta_str = f"{eta_h:02d}:{eta_m:02d}:{eta_s:02d}"
            print(
                f"[ep {current_ep_end}] last return P0={last_final_returns[0]:+.1f}, "
                f"EMA={ema_return:+.3f}, loss={total_loss.item():.3f}, "
                f"eta={eta_str}"
            )

            writer.writerow({
                "ep": current_ep_end,
                "last_return_P0": float(last_final_returns[0]),
                "ema_return": float(ema_return),
                "loss": float(total_loss.item()),
                "eta": eta_str,
            })
            log_f.flush()

            # after loop:
            log_f.close()

    return net

if __name__ == "__main__":
    import os
    os.makedirs("models", exist_ok=True)
    os.makedirs("logs", exist_ok=True)
    # Save model
    t = time.localtime()
    str_t = f"{t.tm_year}-{t.tm_mon:02d}-{t.tm_mday:02d}_{t.tm_hour:02d}-{t.tm_min:02d}-{t.tm_sec:02d}"
    
    # Run training
    net = train(
        n_episodes=2_500_000, 
        lr=3e-4, 
        entropy_coef=0.01,
        value_coef=0.5,
        n_episodes_per_batch=16, # Collect 16 episodes before updating
        hidden=256,
        dropout=0.1
    )
    
    
    
    # Fixed file name for consistency, but added a timestamp
    model_name = f"nmm_a2c_model_{str_t}.pt"
    save_path = os.path.join("models", model_name)
    torch.save(net.state_dict(), save_path)
    print(f"Model saved to {save_path}")